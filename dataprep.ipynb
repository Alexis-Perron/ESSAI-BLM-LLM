{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64d1a932",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.set_option(\"display.max_seq_items\", None)  \n",
    "pd.set_option(\"display.width\", None)         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8750bce4",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data = pd.read_csv(\"C:/Users/alexi/OneDrive/Documents/école/McGill-FIAM/2025/Hackathon-Final-2025/DATA ASSET MANAGEMENT HACKATHON 2025 FINALS/MAIN DATA and SUPPORTING CODES/ret_sample_update.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02942aea",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = raw_data.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f68ae0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for 2005 to 2025, read pickle file get gvkeys into a dictionary and delete the dataframe to free up memory\n",
    "gvkeys_dict = {}\n",
    "for year in range(2005, 2026):\n",
    "    text_data = pd.read_pickle(f\"C:\\\\Users\\\\alexi\\\\OneDrive\\\\Documents\\\\école\\\\McGill-FIAM\\\\2025\\\\Hackathon-Final-2025\\\\DATA ASSET MANAGEMENT HACKATHON 2025 FINALS\\\\TEXT DATA US by YEAR\\\\{year}\\\\text_us_{year}.pkl\")\n",
    "    gvkeys_dict[year] = text_data['gvkey'].unique().tolist()\n",
    "    del text_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98cba08c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For every gvkey in the dictionary, filter the data dataframe to only include rows with those gvkeys for the corresponding year\n",
    "filtered_data_list = []\n",
    "for year in range(2005, 2026):\n",
    "    # data from that year contains only gvkeys in gvkeys_dict[year]\n",
    "    filtered_data = data[(data['year'] == year) & (data['gvkey'].isin(gvkeys_dict[year]))]\n",
    "    filtered_data_list.append(filtered_data)\n",
    "\n",
    "# concatenate all filtered dataframes\n",
    "filtered_data = pd.concat(filtered_data_list, ignore_index=True)\n",
    "del filtered_data_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38ed4756",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep columns: date, excntry, stock_ret, year, month, char_date, market_equity, be_me, ni_me, at_gr1, tangibility, at_be, debt_me, div12m_me, eqpo_me, eqnetis_at, debt_iss, ni_be, profit_sale, gp_at, turnover_126d\n",
    "filtered_data = filtered_data[['date', 'gvkey', 'excntry', 'stock_ret', 'year', 'month', 'char_date', 'market_equity', 'be_me', 'ni_me', 'at_gr1', 'tangibility', 'at_be', 'debt_me', 'div12m_me', 'eqpo_me', 'eqnetis_at', 'dbnetis_at', 'ni_be', 'ebit_sale', 'gp_at', 'turnover_126d']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8da097f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "joining_table = pd.read_csv(\"C:/Users/alexi/OneDrive/Documents/école/McGill-FIAM/2025/Hackathon-Final-2025/DATA ASSET MANAGEMENT HACKATHON 2025 FINALS/MAIN DATA and SUPPORTING CODES/North America Company Name Merge by DataDate-GVKEY-IID.csv\")\n",
    "# rename datadate to date\n",
    "joining_table = joining_table.rename(columns={\"datadate\": \"date\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54b4c24e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Keys côté filtered_data ---\n",
    "# NOTE: char_date dans ret_sample_update est souvent un \"trading month-end\" (ex: 2021-01-29),\n",
    "# alors que joining_table peut contenir la fin de mois calendrier (ex: 2021-01-31).\n",
    "# Pour éviter de perdre des mois entiers au merge, on normalise les deux côtés à la FIN DE MOIS CALENDAIRE.\n",
    "filtered_data['gvkey_key'] = pd.to_numeric(filtered_data['gvkey'], errors='coerce').astype('Int64')\n",
    "\n",
    "filtered_data['char_date_key'] = (\n",
    "    pd.to_datetime(filtered_data['char_date'].astype(str), format='%Y%m%d', errors='coerce')\n",
    "      .dt.to_period('M')\n",
    "      .dt.to_timestamp('M')   # month-end calendrier\n",
    ")\n",
    "\n",
    "# --- Keys côté joining_table ---\n",
    "joining_table['gvkey_key'] = pd.to_numeric(joining_table['gvkey'], errors='coerce').astype('Int64')\n",
    "joining_table['char_date_key'] = (\n",
    "    pd.to_datetime(joining_table['date'], errors='coerce')\n",
    "      .dt.to_period('M')\n",
    "      .dt.to_timestamp('M')   # month-end calendrier\n",
    ")\n",
    "\n",
    "# (optionnel) éviter les duplications si plusieurs lignes par gvkey-date\n",
    "joining_table = joining_table.drop_duplicates(subset=['gvkey_key', 'char_date_key'])\n",
    "\n",
    "# --- Merge sur gvkey + char_date_key (month-end calendrier) ---\n",
    "filtered_data = filtered_data.merge(\n",
    "    joining_table[['gvkey_key', 'char_date_key', 'tic', 'conm']],\n",
    "    on=['gvkey_key', 'char_date_key'],\n",
    "    how='left'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8d685ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "sector_mapping = pd.read_csv(\"C:/Users/alexi/OneDrive/Documents/école/McGill-FIAM/2025/Hackathon-Final-2025/DATA ASSET MANAGEMENT HACKATHON 2025 FINALS/MAIN DATA and SUPPORTING CODES/Sector Info SIC and GIC codes All Countries to merge by GVKEY and Date.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bd56de1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Keys côté filtered_data ---\n",
    "filtered_data['gvkey_key'] = pd.to_numeric(filtered_data['gvkey'], errors='coerce').astype('Int64')\n",
    "filtered_data['date_key'] = (\n",
    "    pd.to_datetime(filtered_data['date'].astype(str), format='%Y%m%d', errors='coerce')\n",
    "      .dt.to_period('M')\n",
    "      .dt.to_timestamp('M')   # month-end calendrier\n",
    ")\n",
    "\n",
    "# --- Keys sector_mapping ---\n",
    "# Le fichier SIC/GICS contient souvent 'date' (trading month-end) ET parfois 'eom' (calendar month-end).\n",
    "# On privilégie 'eom' si présent, sinon on retombe sur 'date', puis on normalise en month-end calendrier.\n",
    "cols = [c for c in ['gvkey', 'date', 'eom', 'gics', 'sic', 'naics'] if c in sector_mapping.columns]\n",
    "sm = sector_mapping[cols].copy()\n",
    "\n",
    "sm['gvkey_key'] = pd.to_numeric(sm['gvkey'], errors='coerce').astype('Int64')\n",
    "\n",
    "date_col = 'eom' if 'eom' in sm.columns else 'date'\n",
    "# Supporte int YYYYMMDD, string YYYYMMDD, ou YYYY-MM-DD\n",
    "sm['_raw_date'] = sm[date_col].astype(str).str.replace('-', '').str.slice(0, 8)\n",
    "sm['date_key'] = (\n",
    "    pd.to_datetime(sm['_raw_date'], format='%Y%m%d', errors='coerce')\n",
    "      .dt.to_period('M')\n",
    "      .dt.to_timestamp('M')\n",
    ")\n",
    "\n",
    "sm = sm.drop(columns=['_raw_date'])\n",
    "# éviter duplications\n",
    "sm = sm.drop_duplicates(subset=['gvkey_key', 'date_key'])\n",
    "\n",
    "# --- Merge ---\n",
    "filtered_data = (\n",
    "    filtered_data.merge(\n",
    "        sm[['gvkey_key', 'date_key', 'gics', 'sic', 'naics']],\n",
    "        on=['gvkey_key', 'date_key'],\n",
    "        how='left'\n",
    "    )\n",
    "    .drop(columns=['gvkey_key', 'date_key'])\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b389084c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Extraire le code secteur GICS (2 premiers chiffres) à partir du code GICS 8 chiffres\n",
    "# ex: 20101010 -> 20\n",
    "filtered_data['gics_sector_code'] = (\n",
    "    pd.to_numeric(filtered_data['gics'], errors='coerce')\n",
    "      .floordiv(10**6)\n",
    "      .astype('Int64')\n",
    ")\n",
    "\n",
    "# 2) Mapping MSCI / GICS (11 secteurs)\n",
    "gics_sector_map = {\n",
    "    10: \"Energy\",\n",
    "    15: \"Materials\",\n",
    "    20: \"Industrials\",\n",
    "    25: \"Consumer Discretionary\",\n",
    "    30: \"Consumer Staples\",\n",
    "    35: \"Health Care\",\n",
    "    40: \"Financials\",\n",
    "    45: \"Information Technology\",\n",
    "    50: \"Communication Services\",\n",
    "    55: \"Utilities\",\n",
    "    60: \"Real Estate\",\n",
    "}\n",
    "\n",
    "# 3) Ajouter le nom du secteur\n",
    "filtered_data['gics_sector_name'] = filtered_data['gics_sector_code'].map(gics_sector_map)\n",
    "filtered_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a49d54c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_data['tic'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f301f77",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import ast\n",
    "import re\n",
    "\n",
    "# Répertoire (relatif au notebook)\n",
    "SP500_DIR = Path(\"sp500-master/sp500_constituants_2005_2024\")\n",
    "\n",
    "def norm_tic(s):\n",
    "    \"\"\"Normalise un ticker pour matcher les listes S&P500.\"\"\"\n",
    "    if pd.isna(s):\n",
    "        return pd.NA\n",
    "    s = str(s).strip().upper()\n",
    "    # Harmoniser BRK.B vs BRK-B, BF.B vs BF-B, etc. (tes fichiers semblent utiliser '.')\n",
    "    s = s.replace(\"-\", \".\")\n",
    "    return s\n",
    "\n",
    "def parse_ticker_list_cell(x):\n",
    "    \"\"\"Parse une cellule du type \"['A', 'AAPL', ...]\" -> list[str].\"\"\"\n",
    "    if pd.isna(x):\n",
    "        return []\n",
    "    if isinstance(x, list):\n",
    "        return [str(t) for t in x]\n",
    "    s = str(x).strip()\n",
    "    # Essaye literal_eval\n",
    "    try:\n",
    "        v = ast.literal_eval(s)\n",
    "        if isinstance(v, list):\n",
    "            return [str(t) for t in v]\n",
    "    except Exception:\n",
    "        pass\n",
    "    # Fallback regex: tout ce qui ressemble à un token alpha-num/./-\n",
    "    return re.findall(r\"[A-Z0-9\\.\\-]+\", s.upper())\n",
    "\n",
    "# 1) Construire une table (year, tic_norm) à partir des fichiers annuels\n",
    "members = []\n",
    "for year in range(2005, 2025):  # tes fichiers sont 2005..2024\n",
    "    fp = SP500_DIR / f\"{year}-sp500-ticker-list.csv\"\n",
    "    if not fp.exists():\n",
    "        continue\n",
    "    dfy = pd.read_csv(fp)\n",
    "    if 'tickers' not in dfy.columns:\n",
    "        continue\n",
    "\n",
    "    # On prend la dernière ligne (souvent la liste à la fin d'année), sinon concat toutes les lignes\n",
    "    # Ici: concat toutes les lignes, puis unique.\n",
    "    all_tickers = []\n",
    "    for x in dfy['tickers'].dropna().tolist():\n",
    "        all_tickers.extend(parse_ticker_list_cell(x))\n",
    "\n",
    "    all_tickers = [norm_tic(t) for t in all_tickers]\n",
    "    all_tickers = [t for t in all_tickers if pd.notna(t)]\n",
    "\n",
    "    if all_tickers:\n",
    "        members.append(pd.DataFrame({'year': year, 'tic_norm': pd.unique(all_tickers)}))\n",
    "\n",
    "sp500_members = pd.concat(members, ignore_index=True).dropna().drop_duplicates()\n",
    "\n",
    "# 2) Filtrer filtered_data par année (S&P500 de l'année correspondante)\n",
    "fd = filtered_data.copy()\n",
    "fd['tic_norm'] = fd['tic'].map(norm_tic)\n",
    "\n",
    "filtered_data = (\n",
    "    fd.merge(sp500_members, on=['year', 'tic_norm'], how='inner')\n",
    "      .drop(columns=['tic_norm'])\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Diagnostic: vérifier la couverture mensuelle (utile pour détecter des mois entiers qui disparaissent)\n",
    "month_counts = (filtered_data.groupby([\"year\", \"month\"]).size().reset_index(name=\"n_rows\"))\n",
    "display(month_counts[month_counts[\"year\"].between(2020, 2021)].sort_values([\"year\",\"month\"]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fe27143",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forward fill values par gvkey, trié par date pour les colonnes: stock_ret, 'market_equity', 'be_me', 'ni_me', 'at_gr1', 'tangibility', 'at_be', 'debt_me', 'div12m_me', 'eqpo_me', 'eqnetis_at', 'dbnetis_at', 'ni_be', 'ebit_sale', 'gp_at', 'turnover_126d', 'gics', 'sic', 'naics', 'gics_sector_code', 'gics_sector_name'\n",
    "filtered_data = filtered_data.sort_values(by=['gvkey', 'date'])\n",
    "cols_to_ffill = ['stock_ret', 'market_equity', 'be_me', 'ni_me', 'at_gr1', 'tangibility', 'at_be', 'debt_me', 'div12m_me', 'eqpo_me', 'eqnetis_at', 'dbnetis_at', 'ni_be', 'ebit_sale', 'gp_at', 'turnover_126d', 'gics', 'sic', 'naics', 'gics_sector_code', 'gics_sector_name']\n",
    "filtered_data[cols_to_ffill] = (filtered_data.groupby('gvkey')[cols_to_ffill].ffill())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9008247",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Réduction de la taille des données\n",
    "# Garder données entre 2021 et 2023 inclus\n",
    "# Éliminer la moitié des tickers\n",
    "\n",
    "filtered_data = filtered_data[(filtered_data['year'] >= 2021) & (filtered_data['year'] <= 2023)]\n",
    "filtered_data = filtered_data.sort_values(by=['gvkey'])\n",
    "unique_gvkeys = filtered_data['gvkey'].unique()\n",
    "reduced_gvkeys = unique_gvkeys[::2]  # garder un gvkey sur deux\n",
    "filtered_data = filtered_data[filtered_data['gvkey'].isin(reduced_gvkeys)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a24dc049",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Afficher le nombre de tickers unique par année\n",
    "for year in range(2005, 2026):\n",
    "    n_unique_tickers = filtered_data[filtered_data['year'] == year]['gvkey'].nunique()\n",
    "    print(f\"Year {year}: {n_unique_tickers} unique tickers\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04e5086f",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_data.to_csv(\"yfinance/filtered_sp500_data.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e76c3a2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d92794b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_data['date'].dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "735160a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_data.columns"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
