{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "64d1a932",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.set_option(\"display.max_seq_items\", None)  \n",
    "pd.set_option(\"display.width\", None)         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8750bce4",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data = pd.read_csv(\"C:/Users/alexi/OneDrive/Documents/école/McGill-FIAM/2025/Hackathon-Final-2025/DATA ASSET MANAGEMENT HACKATHON 2025 FINALS/MAIN DATA and SUPPORTING CODES/ret_sample_update.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02942aea",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = raw_data.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f68ae0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for 2005 to 2025, read pickle file get gvkeys into a dictionary and delete the dataframe to free up memory\n",
    "gvkeys_dict = {}\n",
    "for year in range(2005, 2026):\n",
    "    text_data = pd.read_pickle(f\"C:\\\\Users\\\\alexi\\\\OneDrive\\\\Documents\\\\école\\\\McGill-FIAM\\\\2025\\\\Hackathon-Final-2025\\\\DATA ASSET MANAGEMENT HACKATHON 2025 FINALS\\\\TEXT DATA US by YEAR\\\\{year}\\\\text_us_{year}.pkl\")\n",
    "    gvkeys_dict[year] = text_data['gvkey'].unique().tolist()\n",
    "    del text_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98cba08c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For every gvkey in the dictionary, filter the data dataframe to only include rows with those gvkeys for the corresponding year\n",
    "filtered_data_list = []\n",
    "for year in range(2005, 2026):\n",
    "    # data from that year contains only gvkeys in gvkeys_dict[year]\n",
    "    filtered_data = data[(data['year'] == year) & (data['gvkey'].isin(gvkeys_dict[year]))]\n",
    "    filtered_data_list.append(filtered_data)\n",
    "\n",
    "# concatenate all filtered dataframes\n",
    "filtered_data = pd.concat(filtered_data_list, ignore_index=True)\n",
    "del filtered_data_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38ed4756",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep columns: date, excntry, stock_ret, year, month, char_date, market_equity, be_me, ni_me, at_gr1, tangibility, at_be, debt_me, div12m_me, eqpo_me, eqnetis_at, debt_iss, ni_be, profit_sale, gp_at, turnover_126d\n",
    "filtered_data = filtered_data[['date', 'gvkey', 'excntry', 'stock_ret', 'year', 'month', 'char_date', 'market_equity', 'be_me', 'ni_me', 'at_gr1', 'tangibility', 'at_be', 'debt_me', 'div12m_me', 'eqpo_me', 'eqnetis_at', 'dbnetis_at', 'ni_be', 'ebit_sale', 'gp_at', 'turnover_126d']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8da097f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "joining_table = pd.read_csv(\"C:/Users/alexi/OneDrive/Documents/école/McGill-FIAM/2025/Hackathon-Final-2025/DATA ASSET MANAGEMENT HACKATHON 2025 FINALS/MAIN DATA and SUPPORTING CODES/North America Company Name Merge by DataDate-GVKEY-IID.csv\")\n",
    "# rename datadate to date\n",
    "joining_table = joining_table.rename(columns={\"datadate\": \"date\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54b4c24e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Ajout de 'tic' et 'conm' de façon robuste (week-ends / jours fériés) ===\n",
    "# Problème: 'char_date' peut tomber sur un week-end / jour férié (ex: 2021-02-28, 2021-05-31),\n",
    "# alors que joining_table est souvent indexée sur le dernier jour *ouvrable* du mois.\n",
    "# Un merge exact sur la date peut donc échouer et créer des mois manquants.\n",
    "#\n",
    "# Solution: pour chaque (gvkey, char_date), prendre la dernière ligne de joining_table\n",
    "# dont joining_table.date <= char_date (as-of join), en utilisant np.searchsorted (robuste, rapide),\n",
    "# au lieu de pd.merge_asof (qui impose une contrainte de tri très stricte).\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "fd = filtered_data.copy()\n",
    "fd[\"_row_id\"] = np.arange(len(fd))\n",
    "\n",
    "# Keys côté filtered_data\n",
    "fd[\"gvkey_key\"] = pd.to_numeric(fd[\"gvkey\"], errors=\"coerce\")\n",
    "fd[\"char_date_key\"] = pd.to_datetime(fd[\"char_date\"].astype(str), format=\"%Y%m%d\", errors=\"coerce\")\n",
    "\n",
    "# Keys côté joining_table\n",
    "jt = joining_table[[\"gvkey\", \"date\", \"tic\", \"conm\"]].copy()\n",
    "jt[\"gvkey_key\"] = pd.to_numeric(jt[\"gvkey\"], errors=\"coerce\")\n",
    "jt[\"jt_date_key\"] = pd.to_datetime(jt[\"date\"], errors=\"coerce\")\n",
    "\n",
    "jt = jt.dropna(subset=[\"gvkey_key\", \"jt_date_key\"]).copy()\n",
    "jt = (jt.sort_values([\"gvkey_key\", \"jt_date_key\"])\n",
    "        .drop_duplicates([\"gvkey_key\", \"jt_date_key\"], keep=\"last\")\n",
    "        .reset_index(drop=True))\n",
    "\n",
    "# Grouper les lignes de joining_table par gvkey pour lookup rapide\n",
    "jt_groups = {k: g.reset_index(drop=True) for k, g in jt.groupby(\"gvkey_key\", sort=False)}\n",
    "\n",
    "def _attach_tic_conm(g: pd.DataFrame) -> pd.DataFrame:\n",
    "    t = jt_groups.get(g.name)\n",
    "    if t is None or t.empty:\n",
    "        g[\"tic\"] = pd.NA\n",
    "        g[\"conm\"] = pd.NA\n",
    "        return g\n",
    "\n",
    "    rdates = t[\"jt_date_key\"].to_numpy()\n",
    "    ldates = g[\"char_date_key\"].to_numpy()\n",
    "\n",
    "    pos = np.searchsorted(rdates, ldates, side=\"right\") - 1\n",
    "\n",
    "    tic = np.full(len(g), pd.NA, dtype=object)\n",
    "    conm = np.full(len(g), pd.NA, dtype=object)\n",
    "\n",
    "    ok = pos >= 0\n",
    "    if ok.any():\n",
    "        sel = pos[ok]\n",
    "        tic[ok] = t.loc[sel, \"tic\"].to_numpy()\n",
    "        conm[ok] = t.loc[sel, \"conm\"].to_numpy()\n",
    "\n",
    "    g[\"tic\"] = tic\n",
    "    g[\"conm\"] = conm\n",
    "    return g\n",
    "\n",
    "invalid_mask = fd[\"gvkey_key\"].isna() | fd[\"char_date_key\"].isna()\n",
    "fd_invalid = fd.loc[invalid_mask].copy()\n",
    "fd_valid   = fd.loc[~invalid_mask].copy()\n",
    "\n",
    "# Tri à l'intérieur de chaque gvkey\n",
    "fd_valid = fd_valid.sort_values([\"gvkey_key\", \"char_date_key\"])\n",
    "fd_valid = (fd_valid.groupby(\"gvkey_key\", group_keys=False, sort=False)\n",
    "                    .apply(_attach_tic_conm))\n",
    "\n",
    "# Recombiner et restaurer l'ordre original\n",
    "out = pd.concat([fd_valid, fd_invalid], ignore_index=True)\n",
    "out = out.sort_values(\"_row_id\").drop(columns=[\"_row_id\"], errors=\"ignore\")\n",
    "\n",
    "# Nettoyage des clés auxiliaires si tu ne veux pas les garder:\n",
    "# out = out.drop(columns=[\"gvkey_key\", \"char_date_key\"], errors=\"ignore\")\n",
    "\n",
    "filtered_data = out\n",
    "\n",
    "print(\"[OK] tic/conm ajoutés via as-of join (searchsorted). Couverture tic:\", filtered_data[\"tic\"].notna().mean())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8d685ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "sector_mapping = pd.read_csv(\"C:/Users/alexi/OneDrive/Documents/école/McGill-FIAM/2025/Hackathon-Final-2025/DATA ASSET MANAGEMENT HACKATHON 2025 FINALS/MAIN DATA and SUPPORTING CODES/Sector Info SIC and GIC codes All Countries to merge by GVKEY and Date.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bd56de1",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_data['gvkey_key'] = pd.to_numeric(filtered_data['gvkey'], errors='coerce').astype('Int64')\n",
    "filtered_data['date_key']  = pd.to_datetime(filtered_data['date'].astype(str), format='%Y%m%d', errors='coerce')\n",
    "\n",
    "# --- Keys sector_mapping (date = YYYYMMDD en int) ---\n",
    "sm = sector_mapping[['gvkey','date','gics','sic','naics']].copy()\n",
    "sm['gvkey_key'] = pd.to_numeric(sm['gvkey'], errors='coerce').astype('Int64')\n",
    "sm['date_key']  = pd.to_datetime(sm['date'].astype(str), format='%Y%m%d', errors='coerce')\n",
    "\n",
    "# (optionnel) éviter les duplications si plusieurs lignes par gvkey-date\n",
    "sm = sm.drop_duplicates(subset=['gvkey_key','date_key'])\n",
    "\n",
    "# --- Merge ---\n",
    "filtered_data = filtered_data.merge(\n",
    "    sm[['gvkey_key','date_key','gics','sic','naics']],\n",
    "    on=['gvkey_key','date_key'],\n",
    "    how='left'\n",
    ").drop(columns=['gvkey_key','date_key'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b389084c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Extraire le code secteur GICS (2 premiers chiffres) à partir du code GICS 8 chiffres\n",
    "# ex: 20101010 -> 20\n",
    "filtered_data['gics_sector_code'] = (\n",
    "    pd.to_numeric(filtered_data['gics'], errors='coerce')\n",
    "      .floordiv(10**6)\n",
    "      .astype('Int64')\n",
    ")\n",
    "\n",
    "# 2) Mapping MSCI / GICS (11 secteurs)\n",
    "gics_sector_map = {\n",
    "    10: \"Energy\",\n",
    "    15: \"Materials\",\n",
    "    20: \"Industrials\",\n",
    "    25: \"Consumer Discretionary\",\n",
    "    30: \"Consumer Staples\",\n",
    "    35: \"Health Care\",\n",
    "    40: \"Financials\",\n",
    "    45: \"Information Technology\",\n",
    "    50: \"Communication Services\",\n",
    "    55: \"Utilities\",\n",
    "    60: \"Real Estate\",\n",
    "}\n",
    "\n",
    "# 3) Ajouter le nom du secteur\n",
    "filtered_data['gics_sector_name'] = filtered_data['gics_sector_code'].map(gics_sector_map)\n",
    "filtered_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f301f77",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import ast\n",
    "import re\n",
    "\n",
    "# Répertoire (relatif au notebook)\n",
    "SP500_DIR = Path(\"sp500-master/sp500_constituants_2005_2024\")\n",
    "\n",
    "def norm_tic(s):\n",
    "    if pd.isna(s):\n",
    "        return pd.NA\n",
    "    s = str(s).strip().upper()\n",
    "    # optionnel: harmoniser BRK.B vs BRK-B, BF.B vs BF-B, etc.\n",
    "    s = s.replace(\"-\", \".\")\n",
    "    return s\n",
    "\n",
    "# 1) Charger tous les fichiers annuels et construire (year, tic_norm)\n",
    "members = []\n",
    "for fp in sorted(SP500_DIR.glob(\"*-sp500-ticker-list.csv\")):\n",
    "    # année depuis le nom du fichier (ex: 2006-sp500-ticker-list.csv)\n",
    "    year = int(fp.name.split(\"-\")[0])\n",
    "\n",
    "    df = pd.read_csv(fp)\n",
    "    # tickers est une string du type \"['A', 'AAPL', ...]\"\n",
    "    tickers = ast.literal_eval(df.loc[0, \"tickers\"]) if isinstance(df.loc[0, \"tickers\"], str) else df.loc[0, \"tickers\"]\n",
    "    tickers = [norm_tic(t) for t in tickers]\n",
    "\n",
    "    members.append(pd.DataFrame({\"year\": year, \"tic_norm\": tickers}))\n",
    "\n",
    "sp500_members = pd.concat(members, ignore_index=True).dropna().drop_duplicates()\n",
    "\n",
    "# 2) Filtrer filtered_data par année (S&P500 de l'année correspondante)\n",
    "fd = filtered_data.copy()\n",
    "fd[\"tic_norm\"] = fd[\"tic\"].map(norm_tic)\n",
    "\n",
    "filtered_data = (\n",
    "    fd.merge(sp500_members, on=[\"year\", \"tic_norm\"], how=\"inner\")\n",
    "      .drop(columns=[\"tic_norm\"])\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fe27143",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forward fill values par gvkey, trié par date pour les colonnes: stock_ret, 'market_equity', 'be_me', 'ni_me', 'at_gr1', 'tangibility', 'at_be', 'debt_me', 'div12m_me', 'eqpo_me', 'eqnetis_at', 'dbnetis_at', 'ni_be', 'ebit_sale', 'gp_at', 'turnover_126d', 'gics', 'sic', 'naics', 'gics_sector_code', 'gics_sector_name'\n",
    "filtered_data = filtered_data.sort_values(by=['gvkey', 'date'])\n",
    "cols_to_ffill = ['stock_ret', 'market_equity', 'be_me', 'ni_me', 'at_gr1', 'tangibility', 'at_be', 'debt_me', 'div12m_me', 'eqpo_me', 'eqnetis_at', 'dbnetis_at', 'ni_be', 'ebit_sale', 'gp_at', 'turnover_126d', 'gics', 'sic', 'naics', 'gics_sector_code', 'gics_sector_name']\n",
    "filtered_data[cols_to_ffill] = (filtered_data.groupby('gvkey')[cols_to_ffill].ffill())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9008247",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Réduction de la taille des données\n",
    "# Garder données entre 2021 et 2023 inclus\n",
    "# Éliminer la moitié des tickers\n",
    "\n",
    "filtered_data = filtered_data[(filtered_data['year'] >= 2021) & (filtered_data['year'] <= 2023)]\n",
    "filtered_data = filtered_data.sort_values(by=['gvkey'])\n",
    "# unique_gvkeys = filtered_data['gvkey'].unique()\n",
    "# reduced_gvkeys = unique_gvkeys[::2]  # garder un gvkey sur deux\n",
    "# filtered_data = filtered_data[filtered_data['gvkey'].isin(reduced_gvkeys)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a24dc049",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Afficher le nombre de tickers unique par année\n",
    "for year in range(2005, 2026):\n",
    "    n_unique_tickers = filtered_data[filtered_data['year'] == year]['gvkey'].nunique()\n",
    "    print(f\"Year {year}: {n_unique_tickers} unique tickers\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Add latest available summarized filing JSON (summary_json) as-of each monthly observation ===\n",
    "# This reads the summarized pickles created by summarize_text_reports_v3.py (e.g., C:\\TEXT DATA US SUMMARIZED\\2021\\text_us_2021.pkl)\n",
    "# and attaches, for each (gvkey, date) in filtered_data, the most recent filing summary_json with report_date <= date.\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# --- Pickle compatibility (some pickles reference numpy._core) ---\n",
    "try:\n",
    "    import numpy._core as _ncore\n",
    "    sys.modules.setdefault(\"numpy._core\", _ncore)\n",
    "    try:\n",
    "        sys.modules.setdefault(\"numpy._core._multiarray_umath\", _ncore._multiarray_umath)\n",
    "    except Exception:\n",
    "        pass\n",
    "except Exception:\n",
    "    import numpy as _np\n",
    "    sys.modules.setdefault(\"numpy._core\", _np.core)\n",
    "    try:\n",
    "        sys.modules.setdefault(\"numpy._core._multiarray_umath\", _np.core._multiarray_umath)\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "TEXT_SUM_ROOT = r\"C:\\TEXT DATA US SUMMARIZED\"\n",
    "\n",
    "def _parse_yyyymmdd(x):\n",
    "    \"\"\"Parse YYYYMMDD-like values (int/str) to pandas.Timestamp.\"\"\"\n",
    "    if pd.isna(x):\n",
    "        return pd.NaT\n",
    "    s = str(x)\n",
    "    s_digits = \"\".join(ch for ch in s if ch.isdigit())\n",
    "    if len(s_digits) == 8:\n",
    "        return pd.to_datetime(s_digits, format=\"%Y%m%d\", errors=\"coerce\")\n",
    "    return pd.to_datetime(s, errors=\"coerce\")\n",
    "\n",
    "# Decide which years to load based on filtered_data\n",
    "min_y = int(pd.to_numeric(filtered_data[\"year\"], errors=\"coerce\").min())\n",
    "max_y = int(pd.to_numeric(filtered_data[\"year\"], errors=\"coerce\").max())\n",
    "\n",
    "parts = []\n",
    "available_years = []\n",
    "for y in range(max(2005, min_y - 1), min(2025, max_y) + 1):\n",
    "    p = os.path.join(TEXT_SUM_ROOT, str(y), f\"text_us_{y}.pkl\")\n",
    "    if os.path.exists(p):\n",
    "        df_y = pd.read_pickle(p)\n",
    "        if \"summary_json\" not in df_y.columns:\n",
    "            continue\n",
    "        available_years.append(y)\n",
    "        tmp = df_y[[\"gvkey\", \"date\", \"summary_json\"]].copy()\n",
    "        tmp[\"gvkey\"] = pd.to_numeric(tmp[\"gvkey\"], errors=\"coerce\")\n",
    "        tmp[\"report_date\"] = tmp[\"date\"].apply(_parse_yyyymmdd)\n",
    "        tmp = tmp.dropna(subset=[\"gvkey\", \"report_date\"]).copy()\n",
    "        parts.append(tmp[[\"gvkey\", \"report_date\", \"summary_json\"]])\n",
    "\n",
    "print(\"Summarized years found:\", available_years)\n",
    "\n",
    "if not parts:\n",
    "    print(\"[WARN] No summarized pickles with summary_json found under:\", TEXT_SUM_ROOT)\n",
    "    if \"summary_json\" not in filtered_data.columns:\n",
    "        filtered_data[\"summary_json\"] = pd.NA\n",
    "else:\n",
    "    filings = pd.concat(parts, ignore_index=True)\n",
    "    filings = filings.sort_values([\"gvkey\", \"report_date\"]).drop_duplicates(subset=[\"gvkey\", \"report_date\"], keep=\"last\")\n",
    "\n",
    "    # Build per-gvkey arrays for fast as-of matching\n",
    "    filing_groups = {}\n",
    "    for gv, g in filings.groupby(\"gvkey\", sort=False):\n",
    "        g = g.sort_values(\"report_date\")\n",
    "        filing_groups[gv] = (\n",
    "            g[\"report_date\"].to_numpy(dtype=\"datetime64[ns]\"),\n",
    "            g[\"summary_json\"].to_numpy(dtype=object),\n",
    "        )\n",
    "\n",
    "    # Prepare filtered_data keys\n",
    "    fd = filtered_data.copy()\n",
    "    fd[\"gvkey_key\"] = pd.to_numeric(fd[\"gvkey\"], errors=\"coerce\")\n",
    "    fd[\"date_dt\"] = fd[\"date\"].apply(_parse_yyyymmdd)\n",
    "    fd[\"__row_id\"] = np.arange(len(fd))\n",
    "\n",
    "    valid = fd.dropna(subset=[\"gvkey_key\", \"date_dt\"])[[\"__row_id\", \"gvkey_key\", \"date_dt\"]].copy()\n",
    "\n",
    "    out = pd.Series(pd.NA, index=fd[\"__row_id\"], dtype=object)\n",
    "\n",
    "    for gv, g in valid.groupby(\"gvkey_key\", sort=False):\n",
    "        bundle = filing_groups.get(gv)\n",
    "        if bundle is None:\n",
    "            continue\n",
    "        rdates, rjson = bundle\n",
    "        ldates = g[\"date_dt\"].to_numpy(dtype=\"datetime64[ns]\")\n",
    "        pos = np.searchsorted(rdates, ldates, side=\"right\") - 1\n",
    "        ok = pos >= 0\n",
    "        if np.any(ok):\n",
    "            rid = g.loc[ok, \"__row_id\"].to_numpy(dtype=int)\n",
    "            out.loc[rid] = rjson[pos[ok]]\n",
    "\n",
    "    fd[\"summary_json\"] = fd[\"__row_id\"].map(out)\n",
    "    fd = fd.drop(columns=[\"gvkey_key\", \"date_dt\", \"__row_id\"], errors=\"ignore\")\n",
    "\n",
    "    filtered_data = fd\n",
    "    print(\"Added column: summary_json\")\n",
    "    print(\"Coverage:\", pd.Series(filtered_data[\"summary_json\"]).notna().mean())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04e5086f",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_data.to_csv(\"yfinance/filtered_sp500_data.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "283686ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>gvkey</th>\n",
       "      <th>excntry</th>\n",
       "      <th>stock_ret</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>char_date</th>\n",
       "      <th>market_equity</th>\n",
       "      <th>be_me</th>\n",
       "      <th>ni_me</th>\n",
       "      <th>...</th>\n",
       "      <th>turnover_126d</th>\n",
       "      <th>char_date_key</th>\n",
       "      <th>tic</th>\n",
       "      <th>conm</th>\n",
       "      <th>gics</th>\n",
       "      <th>sic</th>\n",
       "      <th>naics</th>\n",
       "      <th>gics_sector_code</th>\n",
       "      <th>gics_sector_name</th>\n",
       "      <th>summary_json</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20210129</td>\n",
       "      <td>1045.0</td>\n",
       "      <td>USA</td>\n",
       "      <td>0.088776</td>\n",
       "      <td>2021</td>\n",
       "      <td>1</td>\n",
       "      <td>20201231</td>\n",
       "      <td>9800.739885</td>\n",
       "      <td>0.031038</td>\n",
       "      <td>-0.353953</td>\n",
       "      <td>...</td>\n",
       "      <td>0.141481</td>\n",
       "      <td>2020-12-31</td>\n",
       "      <td>AAL</td>\n",
       "      <td>AMERICAN AIRLINES GROUP INC</td>\n",
       "      <td>20302010.0</td>\n",
       "      <td>4512.0</td>\n",
       "      <td>481111.0</td>\n",
       "      <td>20</td>\n",
       "      <td>Industrials</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20220930</td>\n",
       "      <td>1045.0</td>\n",
       "      <td>USA</td>\n",
       "      <td>-0.073133</td>\n",
       "      <td>2022</td>\n",
       "      <td>9</td>\n",
       "      <td>20220831</td>\n",
       "      <td>8441.499391</td>\n",
       "      <td>0.031038</td>\n",
       "      <td>-0.281704</td>\n",
       "      <td>...</td>\n",
       "      <td>0.057210</td>\n",
       "      <td>2022-08-31</td>\n",
       "      <td>AAL</td>\n",
       "      <td>AMERICAN AIRLINES GROUP INC</td>\n",
       "      <td>20302010.0</td>\n",
       "      <td>4512.0</td>\n",
       "      <td>481111.0</td>\n",
       "      <td>20</td>\n",
       "      <td>Industrials</td>\n",
       "      <td>{\"summary\": \"The SEC filing outlines various r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20221031</td>\n",
       "      <td>1045.0</td>\n",
       "      <td>USA</td>\n",
       "      <td>0.177741</td>\n",
       "      <td>2022</td>\n",
       "      <td>10</td>\n",
       "      <td>20220930</td>\n",
       "      <td>7824.350495</td>\n",
       "      <td>0.031038</td>\n",
       "      <td>-0.303923</td>\n",
       "      <td>...</td>\n",
       "      <td>0.051790</td>\n",
       "      <td>2022-09-30</td>\n",
       "      <td>AAL</td>\n",
       "      <td>AMERICAN AIRLINES GROUP INC</td>\n",
       "      <td>20302010.0</td>\n",
       "      <td>4512.0</td>\n",
       "      <td>481111.0</td>\n",
       "      <td>20</td>\n",
       "      <td>Industrials</td>\n",
       "      <td>{\"summary\": \"The SEC filing outlines various r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20221130</td>\n",
       "      <td>1045.0</td>\n",
       "      <td>USA</td>\n",
       "      <td>0.017630</td>\n",
       "      <td>2022</td>\n",
       "      <td>11</td>\n",
       "      <td>20221031</td>\n",
       "      <td>9215.596378</td>\n",
       "      <td>0.031038</td>\n",
       "      <td>-0.208451</td>\n",
       "      <td>...</td>\n",
       "      <td>0.051799</td>\n",
       "      <td>2022-10-31</td>\n",
       "      <td>AAL</td>\n",
       "      <td>AMERICAN AIRLINES GROUP INC</td>\n",
       "      <td>20302010.0</td>\n",
       "      <td>4512.0</td>\n",
       "      <td>481111.0</td>\n",
       "      <td>20</td>\n",
       "      <td>Industrials</td>\n",
       "      <td>{\"summary\": \"The SEC filing outlines various r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20221230</td>\n",
       "      <td>1045.0</td>\n",
       "      <td>USA</td>\n",
       "      <td>-0.118503</td>\n",
       "      <td>2022</td>\n",
       "      <td>12</td>\n",
       "      <td>20221130</td>\n",
       "      <td>9378.071628</td>\n",
       "      <td>0.031038</td>\n",
       "      <td>-0.204840</td>\n",
       "      <td>...</td>\n",
       "      <td>0.049972</td>\n",
       "      <td>2022-11-30</td>\n",
       "      <td>AAL</td>\n",
       "      <td>AMERICAN AIRLINES GROUP INC</td>\n",
       "      <td>20302010.0</td>\n",
       "      <td>4512.0</td>\n",
       "      <td>481111.0</td>\n",
       "      <td>20</td>\n",
       "      <td>Industrials</td>\n",
       "      <td>{\"summary\": \"The SEC filing outlines various r...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       date   gvkey excntry  stock_ret  year  month  char_date  market_equity  \\\n",
       "0  20210129  1045.0     USA   0.088776  2021      1   20201231    9800.739885   \n",
       "1  20220930  1045.0     USA  -0.073133  2022      9   20220831    8441.499391   \n",
       "2  20221031  1045.0     USA   0.177741  2022     10   20220930    7824.350495   \n",
       "3  20221130  1045.0     USA   0.017630  2022     11   20221031    9215.596378   \n",
       "4  20221230  1045.0     USA  -0.118503  2022     12   20221130    9378.071628   \n",
       "\n",
       "      be_me     ni_me  ...  turnover_126d  char_date_key  tic  \\\n",
       "0  0.031038 -0.353953  ...       0.141481     2020-12-31  AAL   \n",
       "1  0.031038 -0.281704  ...       0.057210     2022-08-31  AAL   \n",
       "2  0.031038 -0.303923  ...       0.051790     2022-09-30  AAL   \n",
       "3  0.031038 -0.208451  ...       0.051799     2022-10-31  AAL   \n",
       "4  0.031038 -0.204840  ...       0.049972     2022-11-30  AAL   \n",
       "\n",
       "                          conm        gics     sic     naics  \\\n",
       "0  AMERICAN AIRLINES GROUP INC  20302010.0  4512.0  481111.0   \n",
       "1  AMERICAN AIRLINES GROUP INC  20302010.0  4512.0  481111.0   \n",
       "2  AMERICAN AIRLINES GROUP INC  20302010.0  4512.0  481111.0   \n",
       "3  AMERICAN AIRLINES GROUP INC  20302010.0  4512.0  481111.0   \n",
       "4  AMERICAN AIRLINES GROUP INC  20302010.0  4512.0  481111.0   \n",
       "\n",
       "   gics_sector_code  gics_sector_name  \\\n",
       "0                20       Industrials   \n",
       "1                20       Industrials   \n",
       "2                20       Industrials   \n",
       "3                20       Industrials   \n",
       "4                20       Industrials   \n",
       "\n",
       "                                        summary_json  \n",
       "0                                                NaN  \n",
       "1  {\"summary\": \"The SEC filing outlines various r...  \n",
       "2  {\"summary\": \"The SEC filing outlines various r...  \n",
       "3  {\"summary\": \"The SEC filing outlines various r...  \n",
       "4  {\"summary\": \"The SEC filing outlines various r...  \n",
       "\n",
       "[5 rows x 31 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_data = pd.read_csv(\"yfinance/filtered_sp500_data.csv\")\n",
    "filtered_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d92794b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_data['date'].dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "735160a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_data.columns"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
