{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "import calendar\n",
    "\n",
    "# =========================\n",
    "# Settings (2021-01-01 to 2022-06-30)\n",
    "# =========================\n",
    "START = \"2021-01-01\"\n",
    "END   = \"2023-06-30\"\n",
    "\n",
    "# Month starts: 2021-01-01 ... 2022-06-01\n",
    "date_range = pd.date_range(start=START, end=END, freq=\"MS\")\n",
    "\n",
    "# Store GPT views\n",
    "gpt_means, gpt_vars = [], []\n",
    "dates = []            # month label (YYYY-MM-01)\n",
    "sp500_tickers = None  # ticker universe (fixed from first month)\n",
    "\n",
    "def _month_end(dt: pd.Timestamp) -> str:\n",
    "    last_day = calendar.monthrange(dt.year, dt.month)[1]\n",
    "    return dt.replace(day=last_day).strftime(\"%Y-%m-%d\")\n",
    "\n",
    "for current_date in date_range:\n",
    "    start_date = current_date.strftime(\"%Y-%m-%d\")\n",
    "    end_date = _month_end(current_date)\n",
    "\n",
    "    fp = Path(f\"responses/gpt_{start_date}_{end_date}.json\")\n",
    "    if not fp.exists():\n",
    "        raise FileNotFoundError(f\"Missing GPT responses file: {fp}\")\n",
    "\n",
    "    gpt_dict = json.load(open(fp, \"r\"))\n",
    "\n",
    "    # Fix ticker universe on first month (stable ordering)\n",
    "    if sp500_tickers is None:\n",
    "        sp500_tickers = list(gpt_dict.keys())\n",
    "\n",
    "    means_row, vars_row = [], []\n",
    "    for t in sp500_tickers:\n",
    "        vals = (gpt_dict.get(t, {}) or {}).get(\"expected_return\", None)\n",
    "        if isinstance(vals, list) and len(vals) > 0:\n",
    "            means_row.append(float(np.mean(vals)))\n",
    "            vars_row.append(float(np.var(vals)))\n",
    "        else:\n",
    "            means_row.append(np.nan)\n",
    "            vars_row.append(np.nan)\n",
    "\n",
    "    gpt_means.append(means_row)\n",
    "    gpt_vars.append(vars_row)\n",
    "    dates.append(start_date)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Views (means/vars) as DataFrames: rows=tickers, columns=month-start dates\n",
    "gpt_means_df = pd.DataFrame(np.array(gpt_means), index=dates, columns=sp500_tickers).T\n",
    "gpt_vars_df  = pd.DataFrame(np.array(gpt_vars),  index=dates, columns=sp500_tickers).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baselines combinés :\n",
      " - responses_portfolios\\equal_weighted_portfolio.csv ( 30 rows )\n",
      " - responses_portfolios\\optimized_portfolio.csv ( 30 rows )\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Portfolio_Return</th>\n",
       "      <th>date_key</th>\n",
       "      <th>source_file</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.061814</td>\n",
       "      <td>2021-02-01</td>\n",
       "      <td>equal_weighted_portfolio_2021-01-01_2021-01-31...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.058395</td>\n",
       "      <td>2021-03-01</td>\n",
       "      <td>equal_weighted_portfolio_2021-02-01_2021-02-28...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.046023</td>\n",
       "      <td>2021-04-01</td>\n",
       "      <td>equal_weighted_portfolio_2021-03-01_2021-03-31...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.018338</td>\n",
       "      <td>2021-05-01</td>\n",
       "      <td>equal_weighted_portfolio_2021-04-01_2021-04-30...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000543</td>\n",
       "      <td>2021-06-01</td>\n",
       "      <td>equal_weighted_portfolio_2021-05-01_2021-05-31...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Portfolio_Return   date_key  \\\n",
       "0          0.061814 2021-02-01   \n",
       "1          0.058395 2021-03-01   \n",
       "2          0.046023 2021-04-01   \n",
       "3          0.018338 2021-05-01   \n",
       "4          0.000543 2021-06-01   \n",
       "\n",
       "                                         source_file  \n",
       "0  equal_weighted_portfolio_2021-01-01_2021-01-31...  \n",
       "1  equal_weighted_portfolio_2021-02-01_2021-02-28...  \n",
       "2  equal_weighted_portfolio_2021-03-01_2021-03-31...  \n",
       "3  equal_weighted_portfolio_2021-04-01_2021-04-30...  \n",
       "4  equal_weighted_portfolio_2021-05-01_2021-05-31...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Portfolio_Return</th>\n",
       "      <th>date_key</th>\n",
       "      <th>source_file</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.061814</td>\n",
       "      <td>2021-02-01</td>\n",
       "      <td>optimized_portfolio_2021-01-01_2021-01-31.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.058395</td>\n",
       "      <td>2021-03-01</td>\n",
       "      <td>optimized_portfolio_2021-02-01_2021-02-28.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.046023</td>\n",
       "      <td>2021-04-01</td>\n",
       "      <td>optimized_portfolio_2021-03-01_2021-03-31.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.018338</td>\n",
       "      <td>2021-05-01</td>\n",
       "      <td>optimized_portfolio_2021-04-01_2021-04-30.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000543</td>\n",
       "      <td>2021-06-01</td>\n",
       "      <td>optimized_portfolio_2021-05-01_2021-05-31.csv</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Portfolio_Return   date_key                                    source_file\n",
       "0          0.061814 2021-02-01  optimized_portfolio_2021-01-01_2021-01-31.csv\n",
       "1          0.058395 2021-03-01  optimized_portfolio_2021-02-01_2021-02-28.csv\n",
       "2          0.046023 2021-04-01  optimized_portfolio_2021-03-01_2021-03-31.csv\n",
       "3          0.018338 2021-05-01  optimized_portfolio_2021-04-01_2021-04-30.csv\n",
       "4          0.000543 2021-06-01  optimized_portfolio_2021-05-01_2021-05-31.csv"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "\n",
    "PORT_DIR = \"responses_portfolios\"\n",
    "\n",
    "def _find_date_col(df: pd.DataFrame) -> str:\n",
    "    for c in [\"date_key\", \"Date\", \"date\", \"ym\"]:\n",
    "        if c in df.columns:\n",
    "            return c\n",
    "    # fallback: first unnamed index column\n",
    "    if len(df.columns) > 0 and str(df.columns[0]).lower().startswith(\"unnamed\"):\n",
    "        df = df.rename(columns={df.columns[0]: \"date_key\"})\n",
    "        return \"date_key\"\n",
    "    raise ValueError(f\"Aucune colonne de date trouvée. Colonnes: {list(df.columns)}\")\n",
    "\n",
    "def _read_and_standardize(path: str) -> pd.DataFrame:\n",
    "    df = pd.read_csv(path)\n",
    "    date_col = _find_date_col(df)\n",
    "\n",
    "    # Standardize to 'date_key' as datetime\n",
    "    if date_col == \"ym\":\n",
    "        # 'YYYY-MM' -> month start\n",
    "        df[\"date_key\"] = pd.to_datetime(df[\"ym\"].astype(str) + \"-01\", errors=\"coerce\")\n",
    "    else:\n",
    "        df[\"date_key\"] = pd.to_datetime(df[date_col], errors=\"coerce\")\n",
    "\n",
    "    # drop original date cols (except date_key)\n",
    "    drop_cols = [c for c in [\"Date\", \"date\", \"ym\"] if c in df.columns and c != \"date_key\"]\n",
    "    df = df.drop(columns=drop_cols, errors=\"ignore\")\n",
    "\n",
    "    # keep only rows with date\n",
    "    df = df.dropna(subset=[\"date_key\"]).copy()\n",
    "    return df\n",
    "\n",
    "def combine_monthly_portfolios(\n",
    "    pattern: str,\n",
    "    out_path: str,\n",
    ") -> pd.DataFrame:\n",
    "    files = sorted(glob.glob(os.path.join(PORT_DIR, pattern)))\n",
    "    if not files:\n",
    "        raise FileNotFoundError(f\"Aucun fichier trouvé avec le pattern: {PORT_DIR}/{pattern}\")\n",
    "\n",
    "    dfs = []\n",
    "    for f in files:\n",
    "        df = _read_and_standardize(f)\n",
    "        df[\"source_file\"] = os.path.basename(f)  # pratique pour debug\n",
    "        dfs.append(df)\n",
    "\n",
    "    out = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "    # sort & drop duplicates by date (keep last)\n",
    "    out = out.sort_values(\"date_key\")\n",
    "    out = out.drop_duplicates(subset=[\"date_key\"], keep=\"last\").reset_index(drop=True)\n",
    "\n",
    "    # save combined\n",
    "    os.makedirs(os.path.dirname(out_path), exist_ok=True)\n",
    "    out.to_csv(out_path, index=False)\n",
    "    return out\n",
    "\n",
    "# ---- Build combined baseline files ----\n",
    "eq_combined = combine_monthly_portfolios(\n",
    "    pattern=\"equal_weighted_portfolio*.csv\",\n",
    "    out_path=os.path.join(PORT_DIR, \"equal_weighted_portfolio.csv\"),\n",
    ")\n",
    "\n",
    "opt_combined = combine_monthly_portfolios(\n",
    "    pattern=\"optimized_portfolio*.csv\",\n",
    "    out_path=os.path.join(PORT_DIR, \"optimized_portfolio.csv\"),\n",
    ")\n",
    "\n",
    "print(\"Baselines combinés :\")\n",
    "print(\" -\", os.path.join(PORT_DIR, \"equal_weighted_portfolio.csv\"), \"(\", len(eq_combined), \"rows )\")\n",
    "print(\" -\", os.path.join(PORT_DIR, \"optimized_portfolio.csv\"), \"(\", len(opt_combined), \"rows )\")\n",
    "display(eq_combined.head())\n",
    "display(opt_combined.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Portfolio_Return</th>\n",
       "      <th>date_key</th>\n",
       "      <th>source_file</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.061814</td>\n",
       "      <td>2021-02-01</td>\n",
       "      <td>equal_weighted_portfolio_2021-01-01_2021-01-31...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.058395</td>\n",
       "      <td>2021-03-01</td>\n",
       "      <td>equal_weighted_portfolio_2021-02-01_2021-02-28...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.046023</td>\n",
       "      <td>2021-04-01</td>\n",
       "      <td>equal_weighted_portfolio_2021-03-01_2021-03-31...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.018338</td>\n",
       "      <td>2021-05-01</td>\n",
       "      <td>equal_weighted_portfolio_2021-04-01_2021-04-30...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000543</td>\n",
       "      <td>2021-06-01</td>\n",
       "      <td>equal_weighted_portfolio_2021-05-01_2021-05-31...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Portfolio_Return    date_key  \\\n",
       "0          0.061814  2021-02-01   \n",
       "1          0.058395  2021-03-01   \n",
       "2          0.046023  2021-04-01   \n",
       "3          0.018338  2021-05-01   \n",
       "4          0.000543  2021-06-01   \n",
       "\n",
       "                                         source_file  \n",
       "0  equal_weighted_portfolio_2021-01-01_2021-01-31...  \n",
       "1  equal_weighted_portfolio_2021-02-01_2021-02-28...  \n",
       "2  equal_weighted_portfolio_2021-03-01_2021-03-31...  \n",
       "3  equal_weighted_portfolio_2021-04-01_2021-04-30...  \n",
       "4  equal_weighted_portfolio_2021-05-01_2021-05-31...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\"\"\"\n",
    "Baselines portfolio returns (generated by baselines.py)\n",
    "Expected files:\n",
    "  - responses_portfolios/equal_weighted_portfolio.csv\n",
    "  - responses_portfolios/optimized_portfolio.csv\n",
    "\"\"\"\n",
    "equal_weighted_portfolio_returns = pd.read_csv('responses_portfolios/equal_weighted_portfolio.csv')\n",
    "optimized_portfolio_returns      = pd.read_csv('responses_portfolios/optimized_portfolio.csv')\n",
    "\n",
    "equal_weighted_portfolio_returns.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'results/gpt_black_litterman_weights_tau_0.025.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 10\u001b[39m\n\u001b[32m      7\u001b[39m tau = \u001b[32m0.025\u001b[39m\n\u001b[32m      9\u001b[39m weights_path = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mresults/gpt_black_litterman_weights_tau_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtau\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.csv\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m gpt_weights_df = \u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mweights_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     12\u001b[39m \u001b[38;5;66;03m# detect date column\u001b[39;00m\n\u001b[32m     13\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mDate\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m gpt_weights_df.columns:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\alexi\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1026\u001b[39m, in \u001b[36mread_csv\u001b[39m\u001b[34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[39m\n\u001b[32m   1013\u001b[39m kwds_defaults = _refine_defaults_read(\n\u001b[32m   1014\u001b[39m     dialect,\n\u001b[32m   1015\u001b[39m     delimiter,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1022\u001b[39m     dtype_backend=dtype_backend,\n\u001b[32m   1023\u001b[39m )\n\u001b[32m   1024\u001b[39m kwds.update(kwds_defaults)\n\u001b[32m-> \u001b[39m\u001b[32m1026\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\alexi\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:620\u001b[39m, in \u001b[36m_read\u001b[39m\u001b[34m(filepath_or_buffer, kwds)\u001b[39m\n\u001b[32m    617\u001b[39m _validate_names(kwds.get(\u001b[33m\"\u001b[39m\u001b[33mnames\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[32m    619\u001b[39m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m620\u001b[39m parser = \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    622\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[32m    623\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\alexi\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1620\u001b[39m, in \u001b[36mTextFileReader.__init__\u001b[39m\u001b[34m(self, f, engine, **kwds)\u001b[39m\n\u001b[32m   1617\u001b[39m     \u001b[38;5;28mself\u001b[39m.options[\u001b[33m\"\u001b[39m\u001b[33mhas_index_names\u001b[39m\u001b[33m\"\u001b[39m] = kwds[\u001b[33m\"\u001b[39m\u001b[33mhas_index_names\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m   1619\u001b[39m \u001b[38;5;28mself\u001b[39m.handles: IOHandles | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1620\u001b[39m \u001b[38;5;28mself\u001b[39m._engine = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\alexi\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1880\u001b[39m, in \u001b[36mTextFileReader._make_engine\u001b[39m\u001b[34m(self, f, engine)\u001b[39m\n\u001b[32m   1878\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[32m   1879\u001b[39m         mode += \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1880\u001b[39m \u001b[38;5;28mself\u001b[39m.handles = \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1881\u001b[39m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1882\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1883\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mencoding\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1884\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcompression\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1885\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmemory_map\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1886\u001b[39m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1887\u001b[39m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mencoding_errors\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstrict\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1888\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstorage_options\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1889\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1890\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m.handles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1891\u001b[39m f = \u001b[38;5;28mself\u001b[39m.handles.handle\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\alexi\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pandas\\io\\common.py:873\u001b[39m, in \u001b[36mget_handle\u001b[39m\u001b[34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[39m\n\u001b[32m    868\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[32m    869\u001b[39m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[32m    870\u001b[39m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[32m    871\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m ioargs.encoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs.mode:\n\u001b[32m    872\u001b[39m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m873\u001b[39m         handle = \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[32m    874\u001b[39m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    875\u001b[39m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    876\u001b[39m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[43mioargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    877\u001b[39m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    878\u001b[39m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    879\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    880\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    881\u001b[39m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[32m    882\u001b[39m         handle = \u001b[38;5;28mopen\u001b[39m(handle, ioargs.mode)\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: 'results/gpt_black_litterman_weights_tau_0.025.csv'"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "GPT monthly portfolio weights (Black-Litterman output from evaluate_multiple_updated.py)\n",
    "\"\"\"\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "tau = 0.025\n",
    "\n",
    "weights_path = f\"results/gpt_black_litterman_weights_tau_{tau}.csv\"\n",
    "gpt_weights_df = pd.read_csv(weights_path)\n",
    "\n",
    "# detect date column\n",
    "if \"Date\" in gpt_weights_df.columns:\n",
    "    date_col = \"Date\"\n",
    "elif \"date_key\" in gpt_weights_df.columns:\n",
    "    date_col = \"date_key\"\n",
    "else:\n",
    "    date_col = gpt_weights_df.columns[0]\n",
    "\n",
    "gpt_weights_df[date_col] = pd.to_datetime(gpt_weights_df[date_col], errors=\"coerce\")\n",
    "gpt_weights_df = gpt_weights_df.dropna(subset=[date_col]).set_index(date_col).sort_index()\n",
    "\n",
    "# transpose for plotting: rows=tickers, columns=dates\n",
    "gpt_results = gpt_weights_df.T\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Gemma3 1B monthly portfolio weights (Black-Litterman output from evaluate_multiple.py)\n",
    "\"\"\"\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "tau = 0.025\n",
    "\n",
    "weights_path = f\"results/gemma3_1b_black_litterman_weights_tau_{tau}.csv\"\n",
    "gemma_weights_df = pd.read_csv(weights_path)\n",
    "\n",
    "# detect date column\n",
    "if \"Date\" in gemma_weights_df.columns:\n",
    "    date_col = \"Date\"\n",
    "elif \"date_key\" in gemma_weights_df.columns:\n",
    "    date_col = \"date_key\"\n",
    "else:\n",
    "    date_col = gemma_weights_df.columns[0]\n",
    "\n",
    "gemma_weights_df[date_col] = pd.to_datetime(gemma_weights_df[date_col], errors=\"coerce\")\n",
    "gemma_weights_df = gemma_weights_df.dropna(subset=[date_col]).set_index(date_col).sort_index()\n",
    "\n",
    "# transpose for plotting: rows=tickers, columns=dates\n",
    "gemma_results = gemma_weights_df.T\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Qwen2.5 1.5B monthly portfolio weights (Black-Litterman output)\n",
    "\"\"\"\n",
    "import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "tau = 0.025\n",
    "\n",
    "# Try a couple of common naming conventions\n",
    "patterns = [\n",
    "    f\"results/qwen_black_litterman_weights_tau_{tau}.csv\",\n",
    "    f\"results/qwen_black_litterman_weights_tau_{tau}_*.csv\",\n",
    "]\n",
    "\n",
    "candidates = []\n",
    "for pat in patterns:\n",
    "    candidates += sorted(glob.glob(pat))\n",
    "\n",
    "# If both exact + wildcard matched, keep unique\n",
    "candidates = sorted(set(candidates))\n",
    "if not candidates:\n",
    "    raise FileNotFoundError(\n",
    "        \"Could not find Qwen weights CSV in results/. Tried patterns:\"\n",
    "        + \"\".join(f\"- {p}\" for p in patterns)\n",
    "        + \"Generate it first (e.g., evaluate_multiple / calculate_llm_returns).\"\n",
    "    )\n",
    "\n",
    "qwen_weights_path = candidates[-1]\n",
    "print(\"Using:\", qwen_weights_path)\n",
    "qwen_weights_df = pd.read_csv(qwen_weights_path)\n",
    "\n",
    "# detect date column\n",
    "if \"Date\" in qwen_weights_df.columns:\n",
    "    date_col = \"Date\"\n",
    "elif \"date_key\" in qwen_weights_df.columns:\n",
    "    date_col = \"date_key\"\n",
    "else:\n",
    "    date_col = qwen_weights_df.columns[0]\n",
    "\n",
    "qwen_weights_df[date_col] = pd.to_datetime(qwen_weights_df[date_col], errors=\"coerce\")\n",
    "qwen_weights_df = qwen_weights_df.dropna(subset=[date_col]).set_index(date_col).sort_index()\n",
    "\n",
    "# transpose for plotting: rows=tickers, columns=dates\n",
    "qwen_results = qwen_weights_df.T\n",
    "qwen_results.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "GPT portfolio returns (generated by calculate_llm_returns.py)\n",
    "\"\"\"\n",
    "import glob\n",
    "import pandas as pd\n",
    "\n",
    "tau = 0.025\n",
    "\n",
    "# Match: results/gpt_black_litterman_returns_tau_0.025_2021-01-01_2022-06-30.csv\n",
    "pattern1 = f\"results/gpt_black_litterman_returns_tau_{tau}_*.csv\"\n",
    "pattern2 = f\"results/gpt_black_litterman_returns_tau_{tau}.csv\"\n",
    "\n",
    "candidates = sorted(glob.glob(pattern1))\n",
    "if not candidates:\n",
    "    candidates = sorted(glob.glob(pattern2))\n",
    "\n",
    "if not candidates:\n",
    "    raise FileNotFoundError(\n",
    "        f\"Could not find GPT returns CSV in results/. Tried patterns:\\n- {pattern1}\\n- {pattern2}\\n\"\n",
    "        \"Run calculate_llm_returns.py first.\"\n",
    "    )\n",
    "\n",
    "gpt_returns_path = candidates[-1]\n",
    "print(\"Using:\", gpt_returns_path)\n",
    "gpt_returns = pd.read_csv(gpt_returns_path)\n",
    "gpt_returns.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Gemma3 1B portfolio returns (generated by calculate_llm_returns.py)\n",
    "\"\"\"\n",
    "import glob\n",
    "import pandas as pd\n",
    "\n",
    "tau = 0.025\n",
    "\n",
    "candidates = sorted(glob.glob(f\"results/gemma3_1b_black_litterman_returns_tau_{tau}_*.csv\"))\n",
    "if not candidates:\n",
    "    candidates = sorted(glob.glob(f\"results/gemma3_1b_black_litterman_returns_tau_{tau}.csv\"))\n",
    "\n",
    "if not candidates:\n",
    "    raise FileNotFoundError(\"Could not find Gemma3_1b returns CSV in results/. Run calculate_llm_returns.py first.\")\n",
    "\n",
    "gemma_returns_path = candidates[-1]\n",
    "print(\"Using:\", gemma_returns_path)\n",
    "gemma_returns = pd.read_csv(gemma_returns_path)\n",
    "gemma_returns.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Qwen2.5 1.5B portfolio returns (generated by calculate_llm_returns.py)\n",
    "\"\"\"\n",
    "import glob\n",
    "import pandas as pd\n",
    "\n",
    "tau = 0.025\n",
    "\n",
    "pattern1 = f\"results/qwen_black_litterman_returns_tau_{tau}_*.csv\"\n",
    "pattern2 = f\"results/qwen_black_litterman_returns_tau_{tau}.csv\"\n",
    "\n",
    "candidates = sorted(glob.glob(pattern1))\n",
    "if not candidates:\n",
    "    candidates = sorted(glob.glob(pattern2))\n",
    "\n",
    "if not candidates:\n",
    "    raise FileNotFoundError(\n",
    "        f\"Could not find Qwen returns CSV in results/. Tried patterns:- {pattern1}- {pattern2}\"\n",
    "        \"Run calculate_llm_returns.py first.\"\n",
    "    )\n",
    "\n",
    "qwen_returns_path = candidates[-1]\n",
    "print(\"Using:\", qwen_returns_path)\n",
    "qwen_returns = pd.read_csv(qwen_returns_path)\n",
    "qwen_returns.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "S&P 500 proxy returns (monthly) from yfinance ETF (e.g., SPY):\n",
    "- Download daily adjusted prices\n",
    "- Convert to monthly returns (month-end to month-end)\n",
    "- Keep Jan..Jun 2023 inclusive\n",
    "\"\"\"\n",
    "import pandas as pd\n",
    "\n",
    "try:\n",
    "    import yfinance as yf\n",
    "except ImportError:\n",
    "    raise ImportError(\"yfinance n'est pas installé. Fais: pip install yfinance\")\n",
    "\n",
    "ETF = \"SPY\"  # alternatives: \"IVV\", \"VOO\"\n",
    "\n",
    "px = yf.download(ETF, start=START, end=END, progress=False)\n",
    "if px.empty:\n",
    "    raise ValueError(f\"No data downloaded for {ETF}. Check internet access / ticker.\")\n",
    "\n",
    "# Prefer Adj Close if available, else Close\n",
    "if \"Adj Close\" in px.columns:\n",
    "    close = px[\"Adj Close\"]\n",
    "else:\n",
    "    close = px[\"Close\"]\n",
    "\n",
    "# If DataFrame (edge case), take first column\n",
    "if isinstance(close, pd.DataFrame):\n",
    "    close = close.iloc[:, 0]\n",
    "\n",
    "close = close.dropna()\n",
    "\n",
    "# Month-end close and monthly returns\n",
    "m_close = close.resample(\"ME\").last()\n",
    "m_ret = m_close.pct_change().dropna()\n",
    "\n",
    "sp500_proxy = pd.DataFrame({\n",
    "    \"date_key\": m_ret.index,\n",
    "    \"Portfolio_Return\": m_ret.values\n",
    "})\n",
    "\n",
    "# Keep months end Jan..Jun 2023\n",
    "sp500_proxy = sp500_proxy[(sp500_proxy[\"date_key\"] >= START) & (sp500_proxy[\"date_key\"] <= END)].copy()\n",
    "sp500_proxy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def _standardize_returns(df: pd.DataFrame, name: str) -> pd.Series:\n",
    "    \"\"\"\n",
    "    Accepts df with 'Portfolio_Return' and one date column among: date_key / Date / ym.\n",
    "    Returns a Series indexed by datetime at MONTH START (YYYY-MM-01).\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "\n",
    "    # Detect date column\n",
    "    if \"date_key\" in df.columns:\n",
    "        dt = pd.to_datetime(df[\"date_key\"], errors=\"coerce\")\n",
    "    elif \"Date\" in df.columns:\n",
    "        dt = pd.to_datetime(df[\"Date\"], errors=\"coerce\")\n",
    "    elif \"ym\" in df.columns:\n",
    "        dt = pd.to_datetime(df[\"ym\"].astype(str) + \"-01\", errors=\"coerce\")\n",
    "    else:\n",
    "        raise ValueError(f\"{name}: could not find a date column in {list(df.columns)}\")\n",
    "\n",
    "    if \"Portfolio_Return\" not in df.columns:\n",
    "        raise ValueError(f\"{name}: missing 'Portfolio_Return' column\")\n",
    "\n",
    "    s = pd.Series(pd.to_numeric(df[\"Portfolio_Return\"], errors=\"coerce\").values, index=dt)\n",
    "    s = s.dropna()\n",
    "\n",
    "    # Normalize to month start\n",
    "    s.index = s.index.to_period(\"M\").to_timestamp()\n",
    "\n",
    "    # Deduplicate and sort\n",
    "    s = s[~s.index.duplicated(keep=\"last\")].sort_index()\n",
    "    return s\n",
    "\n",
    "def _infer_periods_per_year(index: pd.DatetimeIndex) -> int:\n",
    "    # For monthly series\n",
    "    return 12\n",
    "\n",
    "# Build return series\n",
    "r_sp500 = _standardize_returns(sp500_proxy, \"SP500 proxy\")\n",
    "r_ew    = _standardize_returns(equal_weighted_portfolio_returns, \"EW\")\n",
    "r_mvo   = _standardize_returns(optimized_portfolio_returns, \"MVO\")\n",
    "r_gpt   = _standardize_returns(gpt_returns, \"BLM-GPT\")\n",
    "r_gemma = _standardize_returns(gemma_returns, \"BLM-Gemma3_1b\")\n",
    "r_qwen  = _standardize_returns(qwen_returns, \"BLM-Qwen2.5_1.5b\")\n",
    "\n",
    "# Align by common dates\n",
    "common_idx = (r_sp500.index\n",
    "    .intersection(r_ew.index)\n",
    "    .intersection(r_mvo.index)\n",
    "    .intersection(r_gpt.index)\n",
    "    .intersection(r_gemma.index)\n",
    "    .intersection(r_qwen.index))\n",
    "\n",
    "r_sp500 = r_sp500.loc[common_idx]\n",
    "r_ew    = r_ew.loc[common_idx]\n",
    "r_mvo   = r_mvo.loc[common_idx]\n",
    "r_gpt   = r_gpt.loc[common_idx]\n",
    "r_gemma = r_gemma.loc[common_idx]\n",
    "r_qwen  = r_qwen.loc[common_idx]\n",
    "\n",
    "portfolios = {\n",
    "    \"SP500\": r_sp500,\n",
    "    \"EW\": r_ew,\n",
    "    \"MVO\": r_mvo,\n",
    "    \"BLM-GPT\": r_gpt,\n",
    "    \"BLM-Gemma3_1b\": r_gemma,\n",
    "    \"BLM-Qwen2.5_1.5b\": r_qwen,\n",
    "}\n",
    "\n",
    "ppy = _infer_periods_per_year(common_idx)\n",
    "rf = 0.02 / ppy  # per-period risk-free rate (2% annual)\n",
    "\n",
    "stats = {}\n",
    "for name, returns in portfolios.items():\n",
    "    returns = returns.astype(float)\n",
    "    cum = (1 + returns).prod() - 1\n",
    "    ann_ret = (1 + cum) ** (ppy / max(1, len(returns))) - 1\n",
    "    ann_vol = returns.std(ddof=1) * np.sqrt(ppy)\n",
    "    sharpe = ((returns.mean() - rf) / returns.std(ddof=1)) * np.sqrt(ppy) if returns.std(ddof=1) > 0 else np.nan\n",
    "\n",
    "    wealth = (1 + returns).cumprod()\n",
    "    peak = wealth.cummax()\n",
    "    dd = wealth / peak - 1\n",
    "    mdd = dd.min()\n",
    "\n",
    "    stats[name] = {\n",
    "        \"Cum. Return\": round(float(cum), 4),\n",
    "        \"Ann. Return\": round(float(ann_ret), 4),\n",
    "        \"Ann. Vol\": round(float(ann_vol), 4),\n",
    "        \"Sharpe\": round(float(sharpe), 4),\n",
    "        \"Max Drawdown\": round(float(mdd), 4),\n",
    "    }\n",
    "\n",
    "stats_df = pd.DataFrame(stats).T\n",
    "print(f\"Periods/year inferred: {ppy}\")\n",
    "stats_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "for name, returns in portfolios.items():\n",
    "    cum = (1 + returns).cumprod()\n",
    "    plt.plot(cum.index, cum.values, label=name)\n",
    "\n",
    "plt.grid(True, linestyle=\"--\", alpha=0.7)\n",
    "plt.xlabel(\"Date\", fontsize=13)\n",
    "plt.ylabel(\"Cumulative Return\", fontsize=13)\n",
    "plt.legend(loc=\"upper left\", fontsize=11)\n",
    "\n",
    "plt.gca().xaxis.set_major_locator(matplotlib.dates.MonthLocator())\n",
    "plt.gca().xaxis.set_major_formatter(matplotlib.dates.DateFormatter(\"%Y-%m\"))\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
